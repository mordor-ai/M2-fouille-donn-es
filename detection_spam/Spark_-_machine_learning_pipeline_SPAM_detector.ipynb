{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '8g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '45G')\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"Python Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, regexp_replace\n",
    "\n",
    "# loading and constructing headers\n",
    "df_spam = spark.read.option(\"header\", True)\\\n",
    "    .csv('./data/spam.csv')\\\n",
    "    .withColumnRenamed(\"v1\", \"label\")\\\n",
    "    .withColumnRenamed(\"v2\", \"sms\")\\\n",
    "    .drop('_c2').drop('_c3').drop('_c4')\\\n",
    "    .withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(label='ham', sms='Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', id=0),\n Row(label='ham', sms='Ok lar... Joking wif u oni...', id=1),\n Row(label='spam', sms=\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", id=2),\n Row(label='ham', sms='U dun say so early hor... U c already then say...', id=3),\n Row(label='ham', sms=\"Nah I don't think he goes to usf, he lives around here though\", id=4)]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF,Tokenizer,  StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+\n",
      "|label|                 sms| id|\n",
      "+-----+--------------------+---+\n",
      "|  0.0|Go until jurong p...|  0|\n",
      "|  0.0|Ok lar Joking wif...|  1|\n",
      "|  1.0|Free entry in 2 a...|  2|\n",
      "|  0.0|U dun say so earl...|  3|\n",
      "|  0.0|Nah I dont think ...|  4|\n",
      "|  1.0|FreeMsg Hey there...|  5|\n",
      "|  0.0|Even my brother i...|  6|\n",
      "|  0.0|As per your reque...|  7|\n",
      "|  1.0|WINNER As a value...|  8|\n",
      "|  1.0|Had your mobile 1...|  9|\n",
      "|  0.0|Im gonna be home ...| 10|\n",
      "|  1.0|SIX chances to wi...| 11|\n",
      "|  1.0|URGENT You have w...| 12|\n",
      "|  0.0|Ive been searchin...| 13|\n",
      "|  0.0|I HAVE A DATE ON ...| 14|\n",
      "|  1.0|XXXMobileMovieClu...| 15|\n",
      "|  0.0|Oh kim watching here| 16|\n",
      "|  0.0|Eh u remember how...| 17|\n",
      "|  0.0|Fine if thats the...| 18|\n",
      "|  1.0|England v Macedon...| 19|\n",
      "+-----+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the labels as 0 and 1 instead of strings\n",
    "# and clean sms txt\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "construct_labels = udf( lambda x: 0.0 if x== 'ham' else 1.0)\n",
    "# Convert to binary label\n",
    "_df_spam = df_spam.withColumn(\"label\", construct_labels(df_spam[\"label\"]).cast('float'))\n",
    "# 0 if not SPAM and 1 if SPAM\n",
    "## retirer les apostrophes et les guillemets\n",
    "_df_spam = _df_spam.withColumn(\"sms\", regexp_replace(\"sms\", \"[^0-9A-Za-z\\\\s]\", \"\"))\\\n",
    "                 .na.fill(\"\")\n",
    "_df_spam.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: float (nullable = true)\n",
      " |-- sms: string (nullable = false)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df_spam.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "# clean text messages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+\n",
      "|label|                 sms|  id|\n",
      "+-----+--------------------+----+\n",
      "|  0.0|                    |3376|\n",
      "|  0.0|                    |4824|\n",
      "|  0.0|   and  picking t...|5486|\n",
      "|  0.0| Am on a train ba...|2677|\n",
      "|  0.0|        Am on my way|2470|\n",
      "|  0.0|  Are you in the pub|5410|\n",
      "|  0.0| Thought I didnt ...|3645|\n",
      "|  0.0| Was a nice day a...|2667|\n",
      "|  0.0| Will be septembe...|1836|\n",
      "|  0.0| Will have two mo...|2218|\n",
      "|  0.0|    all write or wat|4342|\n",
      "|  0.0| and  picking the...|2766|\n",
      "|  0.0| and dont worry w...|2572|\n",
      "|  0.0| anyway many good...|4030|\n",
      "|  0.0|   but your not here|4575|\n",
      "|  0.0| came to look at ...|3673|\n",
      "|  0.0| collecting ur la...|2898|\n",
      "|  0.0| come lt 25 n pas...|3543|\n",
      "|  0.0| comin to fetch u...|4152|\n",
      "|  0.0| dun need to pick...|1174|\n",
      "+-----+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+--------------------+----+\n",
      "|label|                 sms|  id|\n",
      "+-----+--------------------+----+\n",
      "|  0.0| Was really good ...|5034|\n",
      "|  0.0| Was thinking abo...|1752|\n",
      "|  0.0| You gonna ring t...|3248|\n",
      "|  0.0| bot notes oredi ...|4350|\n",
      "|  0.0|    called dad oredi|4617|\n",
      "|  0.0| ok I feel like j...|3037|\n",
      "|  0.0| predict wat time...| 125|\n",
      "|  0.0|  ready then call me|4677|\n",
      "|  0.0|1s finish meeting...|1233|\n",
      "|  0.0|2 and half years ...|4897|\n",
      "|  0.0|2 celebrate my bd...|4446|\n",
      "|  0.0|        26th OF JULY| 991|\n",
      "|  0.0|4 oclock at mine ...| 544|\n",
      "|  0.0|4 tacos  1 rajas ...|3735|\n",
      "|  0.0|A bloo bloo bloo ...|2423|\n",
      "|  0.0|A guy who gets us...|4697|\n",
      "|  0.0|A lot of this sic...|4075|\n",
      "|  0.0|A swt thought Nve...| 143|\n",
      "|  0.0|              ALRITE|2908|\n",
      "|  0.0|ALRITE HUNNYWOT U...|1727|\n",
      "+-----+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "trainingData, testData = _df_spam.randomSplit([0.8, 0.2])\n",
    "trainingData.show()\n",
    "testData.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+\n",
      "|label|                 sms|  id|\n",
      "+-----+--------------------+----+\n",
      "|  1.0|                    |3376|\n",
      "|  1.0|   and  picking t...|5486|\n",
      "|  1.0| Am on a train ba...|2677|\n",
      "|  1.0|  Are you in the pub|5410|\n",
      "|  1.0| FREE POLYPHONIC ...|4903|\n",
      "|  1.0| Was a nice day a...|2667|\n",
      "|  1.0| Was really good ...|5034|\n",
      "|  1.0| Will be septembe...|1836|\n",
      "|  1.0| Will have two mo...|2218|\n",
      "|  1.0| You gonna ring t...|3248|\n",
      "|  1.0|    all write or wat|4342|\n",
      "|  1.0| and  picking the...|2766|\n",
      "|  1.0| anyway many good...|4030|\n",
      "|  1.0| bot notes oredi ...|4350|\n",
      "|  1.0|   but your not here|4575|\n",
      "|  1.0|    called dad oredi|4617|\n",
      "|  1.0| came to look at ...|3673|\n",
      "|  1.0| collecting ur la...|2898|\n",
      "|  1.0| come lt 25 n pas...|3543|\n",
      "|  1.0| dun need to pick...|1174|\n",
      "+-----+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+--------------------+----+\n",
      "|label|                 sms|  id|\n",
      "+-----+--------------------+----+\n",
      "|  1.0|                    |  99|\n",
      "|  1.0|                    |4824|\n",
      "|  1.0|        Am on my way|2470|\n",
      "|  1.0| Thought I didnt ...|3645|\n",
      "|  1.0| Was thinking abo...|1752|\n",
      "|  1.0| and dont worry w...|2572|\n",
      "|  1.0| comin to fetch u...|4152|\n",
      "|  1.0| go home liao Ask...|4941|\n",
      "|  1.0| im  On the snowb...|4387|\n",
      "|  1.0| ltDECIMALgt m bu...|4266|\n",
      "|  1.0| neva tell me how...|3748|\n",
      "|  1.0| yeah Lol Luckily...|2608|\n",
      "|  1.0|07732584351  Rodg...|  42|\n",
      "|  1.0|08714712388 betwe...| 713|\n",
      "|  1.0|09066362231 URGEN...|1463|\n",
      "|  1.0|1 I dont have her...|4369|\n",
      "|  1.0|18 days to Euro20...|1050|\n",
      "|  1.0|1st wk FREE Gr8 t...|3126|\n",
      "|  1.0|2p per min to cal...|5489|\n",
      "|  1.0|4 tacos  1 rajas ...|3735|\n",
      "+-----+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Constructing pipeline\n",
    "\n",
    "tokenizer = Tokenizer ( inputCol=\"sms\" , outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "## Convert to binary label\n",
    "# pas besoin car on a déjà fait\n",
    "indexer = StringIndexer().setInputCol('label').setOutputCol('label')\n",
    "# regression logistique avec pénalités\n",
    "# fonction parcimonieuse => si elle nbe l'est s'adapte à  toutes les situations possibles\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, elasticNetParam=1.)\n",
    "pipeline = Pipeline(stages=[tokenizer , hashingTF , lr ,indexer])"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: float (nullable = true)\n",
      " |-- sms: string (nullable = false)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Fitting the model\n",
    "_model = pipeline.fit(trainingData)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Output column label already exists.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-7b96fb6a8642>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Fitting the model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0m_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrainingData\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/M2-fouille-donn-es/lib/python3.8/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    127\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    128\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 129\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    130\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    131\u001B[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001B[0;32m~/.conda/envs/M2-fouille-donn-es/lib/python3.8/site-packages/pyspark/ml/pipeline.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    107\u001B[0m                     \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    108\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# must be an Estimator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 109\u001B[0;31m                     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    110\u001B[0m                     \u001B[0mtransformers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    111\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mindexOfLastEstimator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/M2-fouille-donn-es/lib/python3.8/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    127\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    128\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 129\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    130\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    131\u001B[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001B[0;32m~/.conda/envs/M2-fouille-donn-es/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    320\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 321\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    322\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    323\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/M2-fouille-donn-es/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    316\u001B[0m         \"\"\"\n\u001B[1;32m    317\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 318\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    320\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/M2-fouille-donn-es/lib/python3.8/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/M2-fouille-donn-es/lib/python3.8/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    135\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 137\u001B[0;31m                 \u001B[0mraise_from\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconverted\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    138\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/M2-fouille-donn-es/lib/python3.8/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mraise_from\u001B[0;34m(e)\u001B[0m\n",
      "\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Output column label already exists."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Evaluation of the model\n",
    "predictions = _model.transform(testData)"
   ],
   "execution_count": 41,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-41-917e48ee2505>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Evaluation of the model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mpredictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtestData\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol('label').setRawPredictionCol('prediction').setMetricName('areaUnderROC')\n",
    "AUC = evaluator.evaluate(predictions)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[['label','sms','prediction','probability','prediction']].filter('label==1').show()\n",
    "\n",
    "# probabilty :  probabilité qu ce n'est pas un spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}